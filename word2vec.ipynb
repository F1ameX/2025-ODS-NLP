{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFSittbMH2wqQ64Y2hZAHF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/F1ameX/2025-ODS-NLP/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice Word2Vec model training"
      ],
      "metadata": {
        "id": "4gpfpJnAzwWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Write a program that trains Word2Vec model. Do not use print() instructions in your code, otherwise test procedure will not succeed; the message \"Wrong Answer\" indicates answer format is incorrect (print() in the code, missing words in the dictionary, etc.). The message \"Embeddings are not good enough\" means you're on the right track and you should focus on the model improvement. In this version of the assignment the checks on the embeddings are easier."
      ],
      "metadata": {
        "id": "0P93X1kn2Nac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TEmkdsPunOgj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def clean(inp: str) -> str:\n",
        "    '''\n",
        "    Cleans the input string by removing punctuation, converting to lowercase,\n",
        "    and replacing multiple spaces with a single space.\n",
        "    '''\n",
        "    inp = inp.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
        "    inp = re.sub(r'\\s+', ' ', inp.lower())\n",
        "    return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lite version of train function"
      ],
      "metadata": {
        "id": "7nbw78AA0iTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data: str) -> dict:\n",
        "    '''\n",
        "    Accepts cleaned data as input and generates word embeddings\n",
        "    using random vectors for each unique word.\n",
        "    '''\n",
        "    w2v_dict = {}\n",
        "\n",
        "    data = clean(data)\n",
        "    words = clean(data).split()\n",
        "\n",
        "    vocab = list(set(words))\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    w2v_dict = {word: np.random.uniform(-0.5, 0.5, vocab_size) for word in vocab}\n",
        "    return w2v_dict"
      ],
      "metadata": {
        "id": "swQNy8c9zUGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hard version of train function"
      ],
      "metadata": {
        "id": "ZzCI1Lnq0n5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data: str) -> dict:\n",
        "    '''\n",
        "    Trains a simple word embedding model using a basic\n",
        "    word-context averaging approach.\n",
        "    '''\n",
        "    window = 2\n",
        "    epochs = 5\n",
        "\n",
        "    words = clean(data).split()\n",
        "    vocab = list(set(words))\n",
        "    vector_size = len(vocab)\n",
        "\n",
        "    w2v_dict = {word: np.random.uniform(-0.5, 0.5, vector_size) for word in vocab}\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for i, word in enumerate(words):\n",
        "            left = max(0, i - window)\n",
        "            right = min(len(words), i + window + 1)\n",
        "            neighbors = words[left : i] + words[i + 1 : right]\n",
        "\n",
        "            if not neighbors:\n",
        "                continue\n",
        "\n",
        "            neighbor_vectors = np.array([w2v_dict[neighbor] for neighbor in neighbors])\n",
        "            avg_vector = np.mean(neighbor_vectors, axis = 0)\n",
        "\n",
        "            w2v_dict[word] = 0.9 * w2v_dict[word] + 0.1 * avg_vector\n",
        "\n",
        "    return w2v_dict"
      ],
      "metadata": {
        "id": "nFfgt5acpH2A"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}